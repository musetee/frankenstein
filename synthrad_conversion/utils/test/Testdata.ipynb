{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_SynthRad_data_2d import *\n",
    "from test_vis_data import *\n",
    "import torch\n",
    "#data_pelvis_path=r'F:\\yang_Projects\\Datasets\\pelvis' #zy7_OLDSERVER\n",
    "data_pelvis_path=r\"C:\\Users\\56991\\Datasets\\Task1\\pelvis\"\n",
    "train_number=5\n",
    "val_number=1\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    ScaleIntensityd,\n",
    "    SqueezeDimd,\n",
    ")\n",
    "train_ds, val_ds=get_file_list(data_pelvis_path, train_number, val_number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_volumes(train_ds, val_ds, saved_name_train=None, saved_name_val=None,resized_size=(600,400,None),ifsave=False,ifcheck=False):\n",
    "    # volume-level transforms for both image and segmentation\n",
    "    train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            NormalizeIntensityd(keys=[\"image\", \"label\"], nonzero=False, channel_wise=True), # z-score normalization\n",
    "            ResizeWithPadOrCropd(keys=[\"image\", \"label\"], spatial_size=resized_size,mode=\"minimum\"),\n",
    "            Rotate90d(keys=[\"image\", \"label\"], k=3),\n",
    "            DivisiblePadd([\"image\", \"label\"], 16, mode=\"minimum\")\n",
    "        ]\n",
    "    )\n",
    "    train_volume_ds = monai.data.CacheDataset(data=train_ds, transform=train_transforms)\n",
    "    val_volume_ds = monai.data.CacheDataset(data=val_ds, transform=train_transforms)\n",
    "    if ifcheck:\n",
    "        # use batch_size=1 to check the volumes because the input volumes have different shapes\n",
    "        train_loader = DataLoader(train_volume_ds, batch_size=1)\n",
    "        # use batch_size=1 to check the volumes because the input volumes have different shapes\n",
    "        val_loader = DataLoader(val_volume_ds, batch_size=1)\n",
    "        train_data = monai.utils.misc.first(train_loader)\n",
    "        val_data = monai.utils.misc.first(val_loader)\n",
    "        print(\"first volume's shape: \", train_data[\"image\"].shape, train_data[\"label\"].shape)\n",
    "        print(\"first volume's shape: \", val_data[\"image\"].shape, val_data[\"label\"].shape)\n",
    "    if ifsave:\n",
    "        torch.save(train_volume_ds, saved_name_train)\n",
    "        torch.save(val_volume_ds, saved_name_val)\n",
    "    return train_volume_ds,val_volume_ds\n",
    "\n",
    "def load_batch_slices(train_volume_ds, val_volume_ds, train_batch_size=5,val_batch_size=1,ifcheck=True):\n",
    "    patch_func = monai.data.PatchIterd(\n",
    "    keys=[\"image\", \"label\"],\n",
    "    patch_size=(None, None, 1),  # dynamic first two dimensions\n",
    "    start_pos=(0, 0, 0)\n",
    "    )\n",
    "    patch_transform = Compose(\n",
    "        [\n",
    "            SqueezeDimd(keys=[\"image\", \"label\"], dim=-1),  # squeeze the last dim\n",
    "        ]\n",
    "    )\n",
    "    # for training\n",
    "    train_patch_ds = monai.data.GridPatchDataset(\n",
    "        data=train_volume_ds, patch_iter=patch_func, transform=patch_transform, with_coordinates=False)\n",
    "    train_loader = DataLoader(\n",
    "        train_patch_ds,\n",
    "        batch_size=train_batch_size,\n",
    "        num_workers=2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    # for validation\n",
    "    val_patch_ds = monai.data.GridPatchDataset(\n",
    "        data=val_volume_ds, patch_iter=patch_func, transform=patch_transform, with_coordinates=False)\n",
    "    val_loader = DataLoader(\n",
    "        val_patch_ds,\n",
    "        batch_size=val_batch_size,\n",
    "        num_workers=2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    if ifcheck:\n",
    "        train_check_data = monai.utils.misc.first(train_loader)\n",
    "        print(train_check_data[\"image\"].shape, train_check_data[\"label\"].shape)\n",
    "        val_check_data = monai.utils.misc.first(val_loader)\n",
    "        print(val_check_data[\"image\"].shape, val_check_data[\"label\"].shape)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 5/5 [00:21<00:00,  4.29s/it]\n",
      "Loading dataset: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "train_volume_ds, val_volume_ds = load_volumes(train_ds, val_ds,saved_name_train=None, saved_name_val=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 400, 608]) torch.Size([5, 1, 400, 608])\n",
      "torch.Size([1, 1, 400, 608]) torch.Size([1, 1, 400, 608])\n"
     ]
    }
   ],
   "source": [
    "train_loader_slices, val_loader_slices=load_batch_slices(train_volume_ds, val_volume_ds, train_batch_size=5,val_batch_size=1,ifcheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "batch_num=0\n",
    "for train_batches in train_loader_slices:\n",
    "    batch_num+=1\n",
    "print(batch_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test transforms and inversed transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_SynthRad_data_2d import *\n",
    "from pix2pix_slices_net import *\n",
    "#data_pelvis_path=r'C:\\Users\\56991\\Datasets\\Task1\\pelvis'\n",
    "data_pelvis_path=r'F:\\yang_Projects\\Datasets\\pelvis'\n",
    "batch_size=8\n",
    "val_batch_size=1\n",
    "train_number=50\n",
    "val_number=5\n",
    "val_interval=1 # every epoch output validation output and save model\n",
    "batch_interval=20 # every batches output training output \n",
    "num_epochs=50\n",
    "\n",
    "model_name='pix2pix'\n",
    "learning_rate=1e-3  #1e-3\n",
    "num_channels=(8,16,32,64,128) #(4,8,16,32) #(8,16,32,64,128) # notice k=(32,32,None) in DivisiblePadd\n",
    "strides= (2, 2, 2, 2) # (2, 2, 2, 2) or (1, 1, 1, 1) \n",
    "loss='BCELoss' # 'BCELoss' or 'SSIMLoss'\n",
    "# choose GPU 0 or 1\n",
    "GPU_ID = 1\n",
    "device = torch.device(f'cuda:{GPU_ID}' if torch.cuda.is_available() else 'cpu') # 0=TitanXP, 1=P5000\n",
    "print(torch.cuda.get_device_name(GPU_ID))\n",
    "\n",
    "Pix2Pixtrainer_2D=Pix2Pixtrainer(learning_rate, num_channels, strides, loss,device, model_name)\n",
    "val_loader=Pix2Pixtrainer_2D.val_loader\n",
    "gen=Pix2Pixtrainer_2D.gen\n",
    "epoch=0\n",
    "val_transforms = Pix2Pixtrainer_2D.train_transforms\n",
    "with torch.no_grad():\n",
    "    val_images = None\n",
    "    val_labels = None\n",
    "    val_output = None\n",
    "    val_ssim=0\n",
    "    val_ssim_sum=0\n",
    "    val_step=0\n",
    "    print(len(val_loader))\n",
    "    for val_data in val_loader:\n",
    "        val_step+=1\n",
    "        val_images, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "        print(val_images.shape)\n",
    "        roi_size = (200, 200)\n",
    "        sw_batch_size = 5\n",
    "        slice_inferer = SliceInferer(\n",
    "            roi_size=roi_size,\n",
    "            sw_batch_size=sw_batch_size,\n",
    "            spatial_dim=2,  # Spatial dim to slice along is defined here\n",
    "            device=device,\n",
    "            padding_mode=\"replicate\",\n",
    "        )\n",
    "        val_output = slice_inferer(val_images, gen)\n",
    "        print(val_output.shape)\n",
    "\n",
    "        val_ssim=calculate_ssim(val_output,val_labels)\n",
    "        val_ssim_sum+=val_ssim\n",
    "        Pix2Pixtrainer_2D.output_val_log(epoch+1, val_step,Pix2Pixtrainer_2D.val_log_file,val_metrices=val_ssim)\n",
    "        \n",
    "        val_output.applied_operations = val_labels.applied_operations\n",
    "        val_output_dict = {\"label\": val_output[0,:,:,:,:]} # always set val_batch_size=1\n",
    "        with allow_missing_keys_mode(val_transforms):\n",
    "            gen_img_volume_dict=val_transforms.inverse(val_output_dict)\n",
    "        gen_img_volume=gen_img_volume_dict[\"label\"]\n",
    "        \n",
    "        file_name_prex=f'pred_epoch_{epoch+1}_valset_{val_step}'\n",
    "        #write_nifti_volume(gen_img_volume,self.saved_img_folder,file_name_prex)\n",
    "        SaveImage(output_dir=Pix2Pixtrainer_2D.saved_img_folder, \\\n",
    "                    output_postfix=file_name_prex,resample=True)(gen_img_volume.detach().cpu())\n",
    "        \n",
    "    # calculate the overall mean ssim of all validation sets\n",
    "    val_ssim_overall=val_ssim/val_step\n",
    "    print(\"val_ssim of epoch %d: %.4f\" % (epoch+1,val_ssim_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms.utils import allow_missing_keys_mode\n",
    "from load_SynthRad_data_2d import *\n",
    "import monai\n",
    "data_pelvis_path=r'C:\\Users\\56991\\Datasets\\Task1\\pelvis'\n",
    "batch_size=8\n",
    "val_batch_size=1\n",
    "train_number=1\n",
    "val_number=2\n",
    "train_volume_ds,val_volume_ds,train_loader,val_loader,train_transforms = \\\n",
    "    myslicesloader(data_pelvis_path,train_number,val_number,batch_size)\n",
    "\n",
    "test_data=monai.utils.misc.first(val_loader)\n",
    "print(test_data[\"image\"].shape)\n",
    "train_check_data=monai.utils.misc.first(train_loader)\n",
    "print(train_check_data[\"image\"].shape)\n",
    "test_data_clone=test_data[\"label\"].clone()\n",
    "test_transforms = train_transforms\n",
    "test_dict_data=test_data_clone[0,:,:,:,:]\n",
    "print(test_dict_data.shape)\n",
    "test_dict = {\"label\": test_dict_data}\n",
    "with allow_missing_keys_mode(test_transforms):\n",
    "   gen_img_volume=test_transforms.inverse(test_dict)\n",
    "print(gen_img_volume[\"label\"].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Save PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Environments\\torch\\venv\\lib\\site-packages\\monai\\utils\\deprecate_utils.py:321: FutureWarning: monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all files in dataset: 180\n"
     ]
    }
   ],
   "source": [
    "from load_SynthRad_data_2d import *\n",
    "#data_pelvis_path=r'C:\\Users\\56991\\Datasets\\Task1\\pelvis'\n",
    "#data_pelvis_path=r'F:\\yang_Projects\\Datasets\\Task1\\pelvis'\n",
    "data_pelvis_path=r'D:\\Projects\\data\\Task1\\pelvis'\n",
    "prefix='png_zscore'\n",
    "saved_img_folder=f'D:\\Projects\\data\\Task1\\{prefix}\\png_images'\n",
    "saved_label_folder=f'D:\\Projects\\data\\Task1\\{prefix}\\png_labels'\n",
    "os.makedirs(saved_img_folder,exist_ok=True)\n",
    "os.makedirs(saved_label_folder,exist_ok=True)\n",
    "model_name='DCGAN' # monai_pix2pix pix2pix WGAN DCGAN\n",
    "batch_size=8\n",
    "val_batch_size=1\n",
    "train_number=2\n",
    "val_number=1\n",
    "train_ds,train_volume_ds=pre_dataset_for_stylegan(data_pelvis_path,\n",
    "                   train_number,\n",
    "                   val_number,\n",
    "                   saved_img_folder,\n",
    "                   saved_label_folder,\n",
    "                   resized_size=(512,512,None),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all files in dataset: 180\n",
      "37064\n"
     ]
    }
   ],
   "source": [
    "def get_file_list(data_pelvis_path, train_number, val_number):\n",
    "    #list all files in the folder\n",
    "    file_list=[i for i in os.listdir(data_pelvis_path) if 'overview' not in i]\n",
    "    file_list_path=[os.path.join(data_pelvis_path,i) for i in file_list]\n",
    "    #list all ct and mr files in folder\n",
    "    ct_file_list=[os.path.join(j,'ct.nii.gz') for j in file_list_path]\n",
    "    mr_file_list=[os.path.join(j,'cbct.nii.gz') for j in file_list_path]\n",
    "    # Dict Version\n",
    "    train_ds = [{'image': i, 'label': j} for i, j in zip(mr_file_list[0:train_number], ct_file_list[0:train_number])]\n",
    "    val_ds = [{'image': i, 'label': j} for i, j in zip(mr_file_list[-val_number:], ct_file_list[-val_number:])]\n",
    "    print('all files in dataset:',len(file_list))\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def sum_slices(data_pelvis_path):\n",
    "    train_ds, val_ds=get_file_list(data_pelvis_path, 0, 180)\n",
    "    train_ds_2d, val_ds_2d,\\\n",
    "    all_slices_train,all_slices_val,\\\n",
    "    shape_list_train,shape_list_val = transform_datasets_to_2d(train_ds, val_ds, \n",
    "                                                            saved_name_train='./train_ds_2d.csv', \n",
    "                                                            saved_name_val='./val_ds_2d.csv', \n",
    "                                                            ifsave=False)\n",
    "    print(all_slices_val)\n",
    "    return all_slices_val\n",
    "\n",
    "data_pelvis_path=r'D:\\Projects\\data\\Task1\\pelvis'\n",
    "data_brain_path=r'D:\\Projects\\data\\Task1\\brain'\n",
    "data_pelvis_path_task2=r'D:\\Projects\\data\\Task2\\pelvis'\n",
    "data_brain_path_task2=r'D:\\Projects\\data\\Task2\\brain'\n",
    "all_slices_train=sum_slices(data_brain_path_task2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Smaller Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\yang_Environments\\torch\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "f:\\yang_Environments\\torch\\venv\\lib\\site-packages\\monai\\utils\\deprecate_utils.py:321: FutureWarning: monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all files in dataset: 180\n",
      "check training data:\n",
      "0 1PA001 image: torch.Size([1, 1, 400, 608, 160]) label: torch.Size([1, 1, 400, 608, 160])\n",
      "checked all training data.\n",
      "check validation data:\n",
      "0 1PC098 image: torch.Size([1, 1, 400, 608, 160]) label: torch.Size([1, 1, 400, 608, 160])\n",
      "checked all validation data.\n"
     ]
    }
   ],
   "source": [
    "from load_SynthRad_data_2d import *\n",
    "data_pelvis_path=r'F:\\yang_Projects\\Datasets\\Task1\\pelvis'\n",
    "\n",
    "batch_size=1\n",
    "val_batch_size=1\n",
    "train_number=1\n",
    "val_number=1\n",
    "resized_size=(600,400,150)\n",
    "div_size=(16,16,16)\n",
    "ifcheck_volume=True\n",
    "# volume-level transforms for both image and segmentation\n",
    "train_transforms = Compose(\n",
    "[\n",
    "    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "    #ScaleIntensityd(keys=[\"image\", \"label\"], minv=0.0, maxv=1.0), # min-max normalization\n",
    "    NormalizeIntensityd(keys=[\"image\", \"label\"], nonzero=False, channel_wise=True), # z-score normalization\n",
    "    ResizeWithPadOrCropd(keys=[\"image\", \"label\"], spatial_size=resized_size,mode=\"minimum\"),\n",
    "    Rotate90d(keys=[\"image\", \"label\"], k=3),\n",
    "    DivisiblePadd([\"image\", \"label\"], k=div_size, mode=\"minimum\")\n",
    "]\n",
    ")\n",
    "train_ds, val_ds = get_file_list(data_pelvis_path, \n",
    "                                train_number, \n",
    "                                val_number)\n",
    "#train_volume_ds, val_volume_ds \n",
    "train_volume_ds,val_volume_ds = load_volumes(train_transforms, \n",
    "                                        train_ds, \n",
    "                                        val_ds, \n",
    "                                        ifsave=False,\n",
    "                                        ifcheck=ifcheck_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_func = monai.data.PatchIterd(\n",
    "    keys=[\"image\", \"label\"],\n",
    "    patch_size=(None, None,40),  # dynamic first two dimensions\n",
    "    start_pos=(0, 0, 0)\n",
    ")\n",
    "\n",
    "# for training\n",
    "train_patch_ds = monai.data.GridPatchDataset(\n",
    "    data=train_volume_ds, patch_iter=patch_func, with_coordinates=False)\n",
    "train_loader = DataLoader(\n",
    "    train_patch_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "val_patch_ds = monai.data.GridPatchDataset(\n",
    "    data=val_volume_ds, patch_iter=patch_func, with_coordinates=False)\n",
    "# for validation\n",
    "val_loader = DataLoader(\n",
    "    val_patch_ds, \n",
    "    num_workers=1, \n",
    "    batch_size=val_batch_size,\n",
    "    pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check train data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n",
      "check train data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n",
      "check train data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n",
      "check train data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n",
      "check val data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n",
      "check val data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n",
      "check val data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n",
      "check val data:\n",
      "image: torch.Size([1, 1, 400, 608, 40]) label: torch.Size([1, 1, 400, 608, 40])\n"
     ]
    }
   ],
   "source": [
    "for idx, train_check_data in enumerate(train_loader):\n",
    "    ds_idx = idx * batch_size\n",
    "    #current_item = train_patch_ds[ds_idx]\n",
    "    print('check train data:')\n",
    "    print('image:', train_check_data['image'].shape, 'label:', train_check_data['label'].shape)\n",
    "\n",
    "for idx, val_check_data in enumerate(val_loader):\n",
    "    ds_idx = idx * val_batch_size\n",
    "    #current_item = val_volume_ds[ds_idx]\n",
    "    print('check val data:')\n",
    "    print('image:', val_check_data['image'].shape, 'label:', val_check_data['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9013 --control=9011 --hb=9010 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"b02f2661-b93c-45e7-a82a-f7587bd6637c\" --shell=9012 --transport=\"tcp\" --iopub=9014 --f=c:\\Users\\zy7\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-757684VP9EagSKTQ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from my_configs1 import cfg as opt\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"StyleGAN pytorch implementation.\")\n",
    "parser.add_argument('--config', default='./configs/sample.yaml')\n",
    "args = parser.parse_args()\n",
    "opt.merge_from_file(args.config)\n",
    "\n",
    "print(opt.path.saved_inference_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
